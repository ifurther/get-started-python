# -*- coding: utf-8 -*-
"""村里戶數、單一年齡人口（新增區域代碼）.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xoNbNQjXAX7mR1QPCrmlXIxB0o7a6_4V
"""

import pandas as pd
import geopandas as gpd
import json
import csv
import requests

#json_url = 'http://data.moi.gov.tw/MoiOD/System/DownloadFile.aspx?DATA=22CFBCD7-63BB-4E58-ADF3-B50BFE55ECED'
json_url = 'https://www.ris.gov.tw/rs-opendata/api/v1/datastore/ODRP014/10701'

data = requests.get(json_url)

population_db_name='population_tw'
client = None
population_db = None

#data.text

#data.text.split(",\n")

if 'VCAP_SERVICES' in os.environ:
    vcap = json.loads(os.getenv('VCAP_SERVICES'))
    print('Found VCAP_SERVICES')
    if 'cloudantNoSQLDB' in vcap:
        creds = vcap['cloudantNoSQLDB'][0]['credentials']
        user = creds['username']
        password = creds['password']
        url = 'https://' + creds['host']
        client = Cloudant(user, password, url=url, connect=True)
        #db = client.create_database(db_name, throw_on_exists=False)
        population_db = client.create_database(population_db_name, throw_on_exists=False)
elif "CLOUDANT_URL" in os.environ:
    client = Cloudant(os.environ['CLOUDANT_USERNAME'], os.environ['CLOUDANT_PASSWORD'], url=os.environ['CLOUDANT_URL'], connect=True)
    #db = client.create_database(db_name, throw_on_exists=False)
    population_db = client.create_database(population_db_name, throw_on_exists=False)
elif os.path.isfile('vcap-local.json'):
    with open('vcap-local.json') as f:
        vcap = json.load(f)
        print('Found local VCAP_SERVICES')
        creds = vcap['services']['cloudantNoSQLDB'][0]['credentials']
        user = creds['username']
        password = creds['password']
        url = 'https://' + creds['host']
        client = Cloudant(user, password, url=url, connect=True)
        #db = client.create_database(db_name, throw_on_exists=False)
        population_db = client.create_database(population_db_name, throw_on_exists=False)
        
def downloadyearmonth(yearbegin=107,yearend=108,monthbegin=1,monthend=12):
  return [ str(y)+"{:02d}".format(m) for y in range(yearbegin,yearend+1) for m in range(monthbegin,monthend+1)]

def getdtatpages(dtaa):
  return range(1,int(''.join(i for i in data.text.split(",\n")[2].split(":")[1] if i.isdigit()))+1)

def searchdata(json_url_noy,data_years):
  data = requests.get(json_url_noy+str(data_years))
  if json.loads(data.text)['responseMessage'] == '查無資料':
    print(data_years,'is not found.')
    return getdtatpages(dtaa)
  else:
    print('The ',data_years,'is downloading.')
    return 0

def getpagedata(json_url_noy,data_years,page,result):
  new_url = json_url_noy+str(data_years)+'?page='+str(page)
  #print(new_url)
  data = requests.get(new_url)
  read_page = int(json.loads(data.text)['page'])
  #print(read_page)
  if page == read_page:
    if len(json.loads(data.text)['responseData']) == int(json.loads(data.text)['pageDataSize']):
      for row in json.loads(data.text)['responseData']:
        district_code = row['district_code']
        district_code = row['district_code'][:-3]
        if district_code == 6500021:
          district_code = 6502100
        totaloldf = 0
        totaoldlm = 0
        oldpeople = 0
        for years in range(65,100):
          totaloldf = int(row['people_age_'+'{:03d}'.format(years)+'_f']) + totaloldf
          totaoldlm = int(row['people_age_'+'{:03d}'.format(years)+'_m']) + totaoldlm
        oldpeople = totaloldf + totaoldlm
        result.append([data_years,district_code,oldpeople])
        return result
    else:
      print('Error Data no.')
  else:
    print('Error page no.')

def get_population_to_db():
    if client:
        return jsonify(list(map(lambda doc: doc['name'], population_db)))
    else:
        print('No database')
        return jsonify([])
        
result = []
json_url_noy = 'https://www.ris.gov.tw/rs-opendata/api/v1/datastore/ODRP014/'


for data_years in downloadyearmonth():
  table_pages = searchdata(json_url_noy,data_years)
  for page in table_pages:
    getpagedata(json_url_noy,data_years,page,result)
      
tt=pd.DataFrame(result)
#tt.columns=['Date','TOWNID','oldpersion']
tt.columns=['Date','TOWNID','p']
#tt.groupby(['Date','TOWNID'])['oldpersion'].aggregate('sum').to_csv('oldpeopledata.csv')
tt.groupby(['Date','TOWNID'])['p'].aggregate('sum')
#tt.to_csv('oldpeopledata.csv')
tt.to_json('oldpeopledata.json')
print(tt.describe())
data = gpd.read_file('TOWN_MOI_1080726.shp',encoding='utf-8')
data['TOWNCODE']=data['TOWNCODE'].astype(int)
dataa = data.merge(ff,on='TOWNCODE')
dataa.to_file('TOWN_MOI_1080726.shp',encoding='utf-8')
print('finished')


